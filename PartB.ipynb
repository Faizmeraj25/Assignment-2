{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing required libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision \nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import datasets, models\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import DataLoader, ConcatDataset, random_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport glob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the pretrained VGG Net16 model","metadata":{}},{"cell_type":"code","source":"vggnet = torchvision.models.vgg16(weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:45:12.226676Z","iopub.execute_input":"2023-04-12T02:45:12.227090Z","iopub.status.idle":"2023-04-12T02:45:20.074349Z","shell.execute_reply.started":"2023-04-12T02:45:12.227055Z","shell.execute_reply":"2023-04-12T02:45:20.073432Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ce84f7bf73b47b491c2bb18b3dbc273"}},"metadata":{}}]},{"cell_type":"code","source":"vggnet","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:45:21.931877Z","iopub.execute_input":"2023-04-12T02:45:21.932954Z","iopub.status.idle":"2023-04-12T02:45:21.941047Z","shell.execute_reply.started":"2023-04-12T02:45:21.932912Z","shell.execute_reply":"2023-04-12T02:45:21.939579Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading the iNaturalist Dataset ","metadata":{}},{"cell_type":"code","source":"def load_dataset(data_augmentation , train_path, test_path, train_batch_size, val_batch_size, test_batch_size):\n    transformer1 = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(), \n        transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])\n    ])\n    train_Dataset = torchvision.datasets.ImageFolder(train_path, transform=transformer1)\n    train_datasize = int(0.8 * len(train_Dataset))\n    train_Dataset, val_Dataset = random_split(train_Dataset, [train_datasize, len(train_Dataset) - train_datasize])\n    if data_augmentation == True: \n        transformer2 = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomHorizontalFlip(0.5), \n        transforms.RandomVerticalFlip(0.02),\n        transforms.RandomRotation(degrees=45),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])\n        ])\n        augmented_dataset = torchvision.datasets.ImageFolder(train_path, transform=transformer2)\n        augmented_dataset_size = int(0.2 * len(augmented_dataset))\n        augmented_dataset, _  =  random_split(augmented_dataset, [augmented_dataset_size, len(augmented_dataset) - augmented_dataset_size])\n        train_Dataset = ConcatDataset([train_Dataset, augmented_dataset])\n    train_Loader = DataLoader(\n        train_Dataset, \n        batch_size = train_batch_size,\n        shuffle=True)\n    test_Loader = DataLoader(\n        test_path,\n        batch_size=test_batch_size, \n        shuffle=True)\n    val_Loader = DataLoader(\n        val_Dataset, \n        batch_size=val_batch_size, \n        shuffle=True)\n    return train_Loader, val_Loader, test_Loader\n\n\ntrain_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/train/'\ntest_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/val/'\ntrain_batch_size = 64\ntest_batch_size = 16\nval_batch_size = 16\nis_Data_Augmentation = True\ntrain_Loader, val_Loader, test_Loader = load_dataset(is_Data_Augmentation, train_path, test_path, train_batch_size, val_batch_size, test_batch_size)\nprint(len(train_Loader), len(val_Loader), len(test_Loader))   ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:45:34.665288Z","iopub.execute_input":"2023-04-12T02:45:34.665757Z","iopub.status.idle":"2023-04-12T02:45:39.370358Z","shell.execute_reply.started":"2023-04-12T02:45:34.665716Z","shell.execute_reply":"2023-04-12T02:45:39.368904Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"157 125 4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Setting device to 'cuda' if GPU is available.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:45:49.466828Z","iopub.execute_input":"2023-04-12T02:45:49.467231Z","iopub.status.idle":"2023-04-12T02:45:49.474803Z","shell.execute_reply.started":"2023-04-12T02:45:49.467192Z","shell.execute_reply":"2023-04-12T02:45:49.473541Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"### Freezing the Model Parameters","metadata":{}},{"cell_type":"code","source":"for param in vggnet.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:45:56.711980Z","iopub.execute_input":"2023-04-12T02:45:56.712420Z","iopub.status.idle":"2023-04-12T02:45:56.718100Z","shell.execute_reply.started":"2023-04-12T02:45:56.712357Z","shell.execute_reply":"2023-04-12T02:45:56.716695Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Adding One more layer to the model.","metadata":{}},{"cell_type":"code","source":"vggnet.classifier[6] = torch.nn.Linear(in_features = 4096, out_features = 10)\nvggnet.classifier.add_module(\"7\", torch.nn.LogSoftmax(dim=1))\nvggnet","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:16.190938Z","iopub.execute_input":"2023-04-12T02:46:16.191409Z","iopub.status.idle":"2023-04-12T02:46:16.202464Z","shell.execute_reply.started":"2023-04-12T02:46:16.191352Z","shell.execute_reply":"2023-04-12T02:46:16.201330Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n    (7): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"vggnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:28.825287Z","iopub.execute_input":"2023-04-12T02:46:28.826605Z","iopub.status.idle":"2023-04-12T02:46:28.838418Z","shell.execute_reply.started":"2023-04-12T02:46:28.826537Z","shell.execute_reply":"2023-04-12T02:46:28.837294Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n    (7): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Function to train the model which logs Train accuracy, Train loss, Validataion accuracy & Validation loss ","metadata":{}},{"cell_type":"code","source":"def train(model, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log): \n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay = 1e-4)\n\n    for epoch in range(epochs):\n        train_accuracy = 0\n        train_loss = 0\n        model.train()\n        for i, (images, labels) in enumerate(train_Loader):\n\n            images, labels = images.to(device), labels.to(device)\n            # doing zero gradient.\n            optimizer.zero_grad()\n\n            #forward Propagation\n            y_pred = model(images)\n\n            # Calculating Loss.\n            loss = loss_function(y_pred, labels)\n\n            # Backward Propagation\n            loss.backward()\n\n            # update rule\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            _, prediction = torch.max(y_pred.data, 1)\n            train_accuracy += int(torch.sum(prediction == labels.data))\n    \n        train_accuracy /= train_count\n        train_loss /= train_count\n        print(f\"Epochs : {epoch+1} Train Accuracy : {train_accuracy} Train Loss {train_loss}\")\n    \n        test_accuracy = 0\n        test_loss = 0\n        with torch.no_grad():\n            model.eval()\n            for i, (images, labels) in enumerate(val_Loader):\n                images, labels = images.to(device), labels.to(device)\n\n                y_pred = model(images)\n\n                loss = loss_function(y_pred, labels)\n                test_loss += loss.item()\n\n                _, predicted = torch.max(y_pred.data, 1)\n\n                test_accuracy += int(torch.sum(predicted == labels.data))\n\n            test_accuracy /= test_count\n            test_loss /= test_count\n\n            print(f\"Epochs : {epoch+1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}\")\n            if(is_wandb_log):\n                wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\" : train_loss, \"val_accuracy\": test_accuracy, \"val_error\": test_loss}) \n            \n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:34.951715Z","iopub.execute_input":"2023-04-12T02:46:34.952122Z","iopub.status.idle":"2023-04-12T02:46:34.966128Z","shell.execute_reply.started":"2023-04-12T02:46:34.952087Z","shell.execute_reply":"2023-04-12T02:46:34.964858Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Return the count of Training dataset & testing dataset","metadata":{}},{"cell_type":"code","source":"def get_train_test_count(train_path, test_path):\n    train_count = len(glob.glob(train_path+'/**/*.jpg'))\n    test_count = len(glob.glob(test_path+'/**/*.jpg'))\n    print(\"Training dataset count : \", train_count)\n    print(\"Validation dataset count\", test_count)\n    return train_count, test_count","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:37.261444Z","iopub.execute_input":"2023-04-12T02:46:37.262583Z","iopub.status.idle":"2023-04-12T02:46:37.268054Z","shell.execute_reply.started":"2023-04-12T02:46:37.262535Z","shell.execute_reply":"2023-04-12T02:46:37.266897Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_count, test_count = get_train_test_count(train_path, test_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:39.355463Z","iopub.execute_input":"2023-04-12T02:46:39.355877Z","iopub.status.idle":"2023-04-12T02:46:40.010250Z","shell.execute_reply.started":"2023-04-12T02:46:39.355838Z","shell.execute_reply":"2023-04-12T02:46:40.009250Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training dataset count :  9999\nValidation dataset count 2000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"import wandb\nlearning_rate = 0.001\nepochs = 8\nis_wandb_log = True\nrun = wandb.init( project='Deep_Learning_Assignment2')\n# run.name = 'ac_' + activation_func + '_nf_' + str(num_filters) + '_ff_'+ str(filter_factor)+'_df_'+str(dropout_factor)\ntrain(vggnet, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:46:50.360948Z","iopub.execute_input":"2023-04-12T02:46:50.361393Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230412_024658-t9gp442n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/t9gp442n' target=\"_blank\">avid-pond-91</a></strong> to <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/t9gp442n' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/t9gp442n</a>"},"metadata":{}}]}]}