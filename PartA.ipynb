{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing required libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision \nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import DataLoader, ConcatDataset, random_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport glob\nimport os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to find mean and standard deviation of DataLoader","metadata":{}},{"cell_type":"code","source":"def get_mean_and_std(dataLoader):\n    mean = 0 \n    std = 0\n    total_image_count = 0\n    for i, (images, labels) in enumerate(dataLoader): \n        batch_image_count = images.size(0)\n        print(batch_image_count)\n        images = images.view(batch_image_count, images.size(1), -1)\n        mean += images.mean(2).sum(0)\n        std += images.std(2).sum(0)\n        total_image_count += batch_image_count\n    mean /= total_image_count\n    std /= total_image_count\n    return mean, std\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-10T16:45:11.728977Z","iopub.execute_input":"2023-04-10T16:45:11.729668Z","iopub.status.idle":"2023-04-10T16:45:11.737893Z","shell.execute_reply.started":"2023-04-10T16:45:11.729622Z","shell.execute_reply":"2023-04-10T16:45:11.736809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Loading the iNatuaralist Dataset","metadata":{}},{"cell_type":"code","source":"def load_dataset(data_augmentation , train_path, test_path, train_batch_size, val_batch_size, test_batch_size):\n    transformer1 = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(), \n        transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])\n    ])\n    train_Dataset = torchvision.datasets.ImageFolder(root=train_path, transform=transformer1)\n    train_datasize = int(0.8 * len(train_Dataset))\n    train_Dataset, val_Dataset = random_split(train_Dataset, [train_datasize, len(train_Dataset) - train_datasize])\n    if data_augmentation == True: \n        transformer2 = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomHorizontalFlip(0.5), \n        transforms.RandomVerticalFlip(0.02),\n        transforms.RandomRotation(degrees=45),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])\n        ])\n        augmented_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=transformer2)\n        augmented_dataset_size = int(0.2 * len(augmented_dataset))\n        augmented_dataset, _  =  random_split(augmented_dataset, [augmented_dataset_size, len(augmented_dataset) - augmented_dataset_size])\n        train_Dataset = ConcatDataset([train_Dataset, augmented_dataset])\n    train_Loader = DataLoader(\n        train_Dataset, \n        batch_size = train_batch_size,\n        shuffle=True)\n    test_Loader = DataLoader(\n        test_path,\n        batch_size=test_batch_size, \n        shuffle=True)\n    val_Loader = DataLoader(\n        val_Dataset, \n        batch_size=val_batch_size, \n        shuffle=True)\n    return train_Loader, val_Loader, test_Loader\n\n\n     \n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:17:44.034534Z","iopub.execute_input":"2023-04-12T02:17:44.035151Z","iopub.status.idle":"2023-04-12T02:17:44.050383Z","shell.execute_reply.started":"2023-04-12T02:17:44.035106Z","shell.execute_reply":"2023-04-12T02:17:44.048993Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Mean = [0.4602, 0.4495, 0.3800] , std = [0.2040, 0.1984, 0.1921] for images of size 256 x 256","metadata":{}},{"cell_type":"markdown","source":"# Storing the Name of the Labels","metadata":{}},{"cell_type":"code","source":"def defineClasses(train_path):\n    classes = []\n    for root,dirs, files in os.walk(train_path):\n    #     for _file in files: \n    #         print(_file)\n        for _dir in dirs:\n            classes.append(_dir)\n    classes.sort()\n    return classes","metadata":{"execution":{"iopub.status.busy":"2023-04-10T10:31:52.221872Z","iopub.execute_input":"2023-04-10T10:31:52.222840Z","iopub.status.idle":"2023-04-10T10:31:52.245094Z","shell.execute_reply.started":"2023-04-10T10:31:52.222793Z","shell.execute_reply":"2023-04-10T10:31:52.241772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting device to cuda if available\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:14.760786Z","iopub.execute_input":"2023-04-12T02:20:14.761302Z","iopub.status.idle":"2023-04-12T02:20:14.770587Z","shell.execute_reply.started":"2023-04-12T02:20:14.761253Z","shell.execute_reply":"2023-04-12T02:20:14.769048Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create a CNN Model","metadata":{}},{"cell_type":"code","source":"class create_lenet(torch.nn.Module):\n    def __init__(self, num_classes, Kernel_size, num_filters, activation_func, filter_factor, is_data_augment, dropout_factor):\n        super(create_lenet, self).__init__()\n        self.actfunc1 = None\n        self.actfunc2 = None\n        self.actfunc3 = None\n        self.actfunc4 = None\n        self.actfunc5 = None\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=num_filters, kernel_size=Kernel_size)\n        self.size = (256 - Kernel_size)\n        prev_number_filters = num_filters\n        number_filters = int(num_filters * filter_factor)\n        if activation_func == 'ReLU':\n            self.actfunc1 = torch.nn.ReLU()\n        elif activation_func == 'GELU':\n            self.actfunc1 = torch.nn.GELU()\n        elif activation_func == 'SiLU':\n            self.actfunc1 = torch.nn.SiLU()\n        elif activation_func == 'Mish':\n            self.actfunc1 = torch.nn.Mish()\n        else:#By Default\n            self.actfunc1 = torch.nn.ReLU()\n        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=Kernel_size, stride=2)\n        self.size = self.size//2\n        self.conv2 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=Kernel_size)\n        self.size = (self.size - Kernel_size) \n        prev_number_filters = number_filters\n        number_filters = int(number_filters * filter_factor)\n        \n        if activation_func == 'ReLU':\n            self.actfunc2 = torch.nn.ReLU()\n        elif activation_func == 'GELU':\n            self.actfunc2 = torch.nn.GELU()\n        elif activation_func == 'SiLU':\n            self.actfunc2 = torch.nn.SiLU()\n        elif activation_func == 'Mish':\n            self.actfunc2 = torch.nn.Mish()\n        else:#By Default\n            self.actfunc2 = torch.nn.ReLU()\n            \n        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=Kernel_size, stride=(2,2))\n        self.size = self.size//2\n        \n        self.conv3 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=Kernel_size)\n        self.size = (self.size - Kernel_size) \n        \n        prev_number_filters = number_filters\n        number_filters = int(number_filters * filter_factor)\n        \n        if activation_func == 'ReLU':\n            self.actfunc3 = torch.nn.ReLU()\n        elif activation_func == 'GELU':\n            self.actfunc3 = torch.nn.GELU()\n        elif activation_func == 'SiLU':\n            self.actfunc3 = torch.nn.SiLU()\n        elif activation_func == 'Mish':\n            self.actfunc3 = torch.nn.Mish()\n        else:#By Default\n            self.actfunc3 = torch.nn.ReLU()\n            \n        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=Kernel_size, stride=2)\n        # 50 x 16 x 16\n        self.size = self.size//2\n        \n        self.conv4 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=Kernel_size)\n        # 50 x 14 x 14\n        self.size = (self.size - Kernel_size) \n        \n        prev_number_filters = number_filters\n        number_filters = int(number_filters * filter_factor)\n        \n        if activation_func == 'ReLU':\n            self.actfunc4 = torch.nn.ReLU()\n        elif activation_func == 'GELU':\n            self.actfunc4 = torch.nn.GELU()\n        elif activation_func == 'SiLU':\n            self.actfunc4 = torch.nn.SiLU()\n        elif activation_func == 'Mish':\n            self.actfunc4 = torch.nn.Mish()\n        else:#By Default\n            self.actfunc4 = torch.nn.ReLU()\n            \n        self.maxpool4 = torch.nn.MaxPool2d(kernel_size=Kernel_size, stride=2)\n        self.size = self.size//2\n        \n        self.conv5 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=Kernel_size)\n        self.size = (self.size - Kernel_size) \n        prev_number_filters = number_filters\n        if activation_func == 'ReLU':\n            self.actfunc5 = torch.nn.ReLU()\n        elif activation_func == 'GELU':\n            self.actfunc5 = torch.nn.GELU()\n        elif activation_func == 'SiLU':\n            self.actfunc5 = torch.nn.SiLU()\n        elif activation_func == 'Mish':\n            self.actfunc5 = torch.nn.Mish()\n        else:#By Default\n            self.actfunc5 = torch.nn.ReLU()\n            \n        self.maxpool5 = torch.nn.MaxPool2d(kernel_size=Kernel_size, stride=2)\n        self.size = self.size//2\n        #Need to calculate the in_features.\n        self.size = self.size * self.size * prev_number_filters\n        self.fc1 = torch.nn.Linear(in_features=self.size, out_features=self.size)\n        self.dropp1 = torch.nn.Dropout(dropout_factor)\n        self.fc2 = torch.nn.Linear(in_features=self.size, out_features=num_classes)\n        self.dropp2 = torch.nn.Dropout(dropout_factor)\n        self.logSoftmax = torch.nn.LogSoftmax(dim=1)\n\n    def forward(self, x): \n        x = self.conv1(x)\n        x = self.actfunc1(x)\n        x = self.maxpool1(x)\n\n        x = self.conv2(x)\n        x = self.actfunc2(x)\n        x = self.maxpool2(x)\n\n        x = self.conv3(x)\n        x = self.actfunc3(x)\n        x = self.maxpool3(x)\n\n        x = self.conv4(x)\n        x = self.actfunc4(x)\n        x = self.maxpool4(x)\n\n        \n        x = self.conv5(x)\n        x = self.actfunc5(x)\n        x = self.maxpool5(x)\n\n        x = x.view(-1,x.shape[1]*x.shape[2] * x.shape[3])\n        x = self.fc1(x)\n        x = self.dropp1(x)\n        x = self.fc2(x)\n        x = self.dropp2(x)\n        output = self.logSoftmax(x)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:19.072484Z","iopub.execute_input":"2023-04-12T02:20:19.072979Z","iopub.status.idle":"2023-04-12T02:20:19.109260Z","shell.execute_reply.started":"2023-04-12T02:20:19.072935Z","shell.execute_reply":"2023-04-12T02:20:19.107590Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Create CNN Model","metadata":{}},{"cell_type":"code","source":"# kernel_size = 2\n# num_classes = len(classes)\n# num_filters = 32\n# activation_func = 'ReLU'\n# filter_factor = 2\n# is_data_augment = True\n# cnn = create_lenet(num_classes, kernel_size, num_filters, activation_func, filter_factor, is_data_augment)\n# cnn","metadata":{"execution":{"iopub.status.busy":"2023-04-10T16:49:05.719473Z","iopub.execute_input":"2023-04-10T16:49:05.720079Z","iopub.status.idle":"2023-04-10T16:49:05.724436Z","shell.execute_reply.started":"2023-04-10T16:49:05.720040Z","shell.execute_reply":"2023-04-10T16:49:05.723421Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Counting the size of train and test dataset","metadata":{}},{"cell_type":"code","source":"def get_train_test_count(train_pat, test_pat):\n    train_count = len(glob.glob(train_path+'/**/*.jpg'))\n    test_count = len(glob.glob(test_path+'/**/*.jpg'))\n    print(\"Training dataset count : \", train_count)\n    print(\"Validation dataset count\", test_count)\n    return train_count, test_count","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:28.091716Z","iopub.execute_input":"2023-04-12T02:20:28.093254Z","iopub.status.idle":"2023-04-12T02:20:28.102519Z","shell.execute_reply.started":"2023-04-12T02:20:28.093186Z","shell.execute_reply":"2023-04-12T02:20:28.100651Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Training the model ","metadata":{}},{"cell_type":"code","source":"\ndef train(cnn, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log): \n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(params=cnn.parameters(), lr=learning_rate, weight_decay = 1e-4)\n\n    for epoch in range(epochs):\n        train_accuracy = 0\n        train_loss = 0\n        cnn.train()\n        for i, (images, labels) in enumerate(train_Loader):\n\n            images, labels = images.to(device), labels.to(device)\n            # doing zero gradient.\n            optimizer.zero_grad()\n\n            #forward Propagation\n            y_pred = cnn(images)\n\n            # Calculating Loss.\n            loss = loss_function(y_pred, labels)\n\n            # Backward Propagation\n            loss.backward()\n\n            # update rule\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            _, prediction = torch.max(y_pred.data, 1)\n            train_accuracy += int(torch.sum(prediction == labels.data))\n    \n        train_accuracy /= train_count\n        train_loss /= train_count\n        print(f\"Epochs : {epoch+1} Train Accuracy : {train_accuracy} Train Loss {train_loss}\")\n    \n        test_accuracy = 0\n        test_loss = 0\n        with torch.no_grad():\n            cnn.eval()\n            for i, (images, labels) in enumerate(val_Loader):\n                images, labels = images.to(device), labels.to(device)\n\n                y_pred = cnn(images)\n\n                loss = loss_function(y_pred, labels)\n                test_loss += loss.item()\n\n                _, predicted = torch.max(y_pred.data, 1)\n\n                test_accuracy += int(torch.sum(predicted == labels.data))\n\n            test_accuracy /= test_count\n            test_loss /= test_count\n\n            print(f\"Epochs : {epoch+1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}\")\n            if(is_wandb_log):\n                wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\" : train_loss, \"val_accuracy\": test_accuracy, \"val_error\": test_loss}) \n            \n\n# learning_rate = 0.001\n# epochs = 3\n# is_wandb_log = True\n# train(learning_rate, epochs, is_wandb_log)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:30.916354Z","iopub.execute_input":"2023-04-12T02:20:30.917814Z","iopub.status.idle":"2023-04-12T02:20:30.932378Z","shell.execute_reply.started":"2023-04-12T02:20:30.917735Z","shell.execute_reply":"2023-04-12T02:20:30.930713Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Function to get the Test Accuracy & Test Loss","metadata":{}},{"cell_type":"code","source":"def test(cnn, num_classes, test_path, test_batch_size , num_filters, kernel_size, activation_func, filter_factor, learning_rate, epochs, dropout_factor):\n    transformer = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor()\n    ])\n        \n    test_Dataset = torchvision.datasets.ImageFolder(root=test_path, transform=transformer)\n    test_Loader = DataLoader(\n    test_Dataset,\n    batch_size=test_batch_size, \n    shuffle=True)\n    test_count = len(glob.glob(test_path+'/**/*.jpg'))\n    print('Testing the Model...')\n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(params=cnn.parameters(), lr=learning_rate)\n    \n    for epoch in range(epochs):\n        test_accuracy = 0\n        test_loss = 0\n        with torch.no_grad():\n            cnn.eval()\n            for i, (images, labels) in enumerate(test_Loader):\n                \n                images, labels = images.to(device), labels.to(device)\n                y_pred = cnn(images)\n\n                loss = loss_function(y_pred, labels)\n                test_loss += loss.item()\n\n                _, predicted = torch.max(y_pred.data, 1)\n\n                test_accuracy += int(torch.sum(predicted == labels.data))\n\n            test_accuracy /= test_count\n            test_loss /= test_count\n\n            print(f\"Epochs : {epoch+1} Test Accuracy : {test_accuracy} Test Loss {test_loss}\")\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:35.250557Z","iopub.execute_input":"2023-04-12T02:20:35.251024Z","iopub.status.idle":"2023-04-12T02:20:35.264313Z","shell.execute_reply.started":"2023-04-12T02:20:35.250984Z","shell.execute_reply":"2023-04-12T02:20:35.262607Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def main(num_classes, kernel_size,  train_path, test_path, train_batch_size, val_batch_size, test_batch_size, num_filters, activation_func, filter_factor, is_data_augment, learning_rate, epochs, is_wandb_log, dropout_factor):\n    print(\"train_path\", train_path)\n    print(\"test_path\", test_path)\n    train_Loader, val_Loader, test_Loader = load_dataset(is_data_augment, train_path, test_path, train_batch_size, val_batch_size, test_batch_size)\n    cnn = create_lenet(num_classes, kernel_size, num_filters, activation_func, filter_factor, is_data_augment, dropout_factor)\n    cnn = cnn.to(device)\n    train_count, test_count = get_train_test_count(train_path, test_path)\n    print(\"Training the Model...\")\n    train(cnn, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log)\n    print(\"Training Finished !!\")\n    return cnn\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:38.736524Z","iopub.execute_input":"2023-04-12T02:20:38.737035Z","iopub.status.idle":"2023-04-12T02:20:38.747479Z","shell.execute_reply.started":"2023-04-12T02:20:38.736990Z","shell.execute_reply":"2023-04-12T02:20:38.746394Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Running Sweep","metadata":{}},{"cell_type":"code","source":"import wandb\ntrain_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/train/'\ntest_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/val/'\ntrain_batch_size = 64\ntest_batch_size = 16\nval_batch_size = 16\nnum_classes = 10\nkernel_size = 3\nis_wandb_log = True\nis_data_augment=True\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:41.850174Z","iopub.execute_input":"2023-04-12T02:20:41.851459Z","iopub.status.idle":"2023-04-12T02:20:42.757011Z","shell.execute_reply.started":"2023-04-12T02:20:41.851389Z","shell.execute_reply":"2023-04-12T02:20:42.755248Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def run_sweep():\n    \n    default_params =dict(\n        num_filters = 32, \n        activation_func = 'ReLU',\n        filter_factor = 2, \n        learning_rate = 0.001,\n        epochs = 3,   \n        dropout_factor = 0,\n    )\n\n    run = wandb.init(config=default_params, project='Deep_Learning_Assignment2')\n\n    config = wandb.config\n    num_filters = config.num_filters\n    activation_func = config.activation_func\n    filter_factor = config.filter_factor\n    learning_rate = config.learning_rate\n    epochs = config.epochs\n    dropout_factor = config.dropout_factor\n\n    run.name = 'ac_' + activation_func + '_nf_' + str(num_filters) + '_ff_'+ str(filter_factor)+'_df_'+str(dropout_factor)\n    main(num_classes, kernel_size,  train_path, test_path, train_batch_size, val_batch_size, test_batch_size, num_filters, activation_func, filter_factor, is_data_augment, learning_rate, epochs, is_wandb_log, dropout_factor)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:20:45.208656Z","iopub.execute_input":"2023-04-12T02:20:45.209255Z","iopub.status.idle":"2023-04-12T02:20:45.222580Z","shell.execute_reply.started":"2023-04-12T02:20:45.209204Z","shell.execute_reply":"2023-04-12T02:20:45.220946Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Set up a sweep config\nsweep_config = {\n    'metric': {\n        'goal': 'maximize', \n        'name': 'val_accuracy'\n        },\n    \"method\": \"bayes\",\n    \"project\": \"Deep_Learning_Assignment2\",\n    \"parameters\": {\n        \"num_filters\": {\n            \"values\": [32, 64]\n        },\n        \"filter_factor\": {\n            \"values\": [0.5]\n        },\n        \"epochs\": {\n            \"values\": [5, 10]\n        },\n        \"activation_func\":\n        {\n            \"values\" : [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n        },\n        \"dropout_factor\":\n        {\n            \"values\": [0 , 0.3, 0.4]\n        },\n    }\n}\n\n# creating the sweep\nsweep_id = wandb.sweep(sweep_config, project=\"Deep_Learning_Assignment2\")","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:27:34.193616Z","iopub.execute_input":"2023-04-11T16:27:34.194076Z","iopub.status.idle":"2023-04-11T16:27:45.881063Z","shell.execute_reply.started":"2023-04-11T16:27:34.194033Z","shell.execute_reply":"2023-04-11T16:27:45.879733Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Create sweep with ID: 9ov2hh6z\nSweep URL: https://wandb.ai/cs22m081/Deep_Learning_Assignment2/sweeps/9ov2hh6z\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.agent(sweep_id, function=run_sweep)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T16:27:45.883232Z","iopub.execute_input":"2023-04-11T16:27:45.883580Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mobxa91w with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: Mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_factor: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_factor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_162750-mobxa91w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/mobxa91w' target=\"_blank\">clean-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/sweeps/9ov2hh6z' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2/sweeps/9ov2hh6z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/sweeps/9ov2hh6z' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2/sweeps/9ov2hh6z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/mobxa91w' target=\"_blank\">https://wandb.ai/cs22m081/Deep_Learning_Assignment2/runs/mobxa91w</a>"},"metadata":{}},{"name":"stdout","text":"train_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/train/\ntest_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/val/\nTraining dataset count :  9999\nValidation dataset count 2000\nTraining the Model...\nEpochs : 1 Train Accuracy : 0.18921892189218922 Train Loss 0.03423354423979614\nEpochs : 1 Validation Accuracy : 0.2315 Validation Loss 0.13015175062417983\nEpochs : 2 Train Accuracy : 0.24992499249924993 Train Loss 0.032213543186021786\nEpochs : 2 Validation Accuracy : 0.2725 Validation Loss 0.1261451278924942\nEpochs : 3 Train Accuracy : 0.2816281628162816 Train Loss 0.03110701081180277\nEpochs : 3 Validation Accuracy : 0.269 Validation Loss 0.12527247768640518\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Run Without sweep","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/train/'\ntest_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/val/'\ntrain_batch_size = 64\ntest_batch_size = 16\nval_batch_size = 16\nnum_classes = 10\nkernel_size = 3\nis_wandb_log = False\nis_data_augment=True\nnum_filters = 32\nactivation_func = 'Mish'\nfilter_factor = 1\nlearning_rate = 0.001\nepochs = 10\ndropout_factor = 0.3\n\nmain(num_classes, kernel_size,  train_path, test_path, train_batch_size, val_batch_size, test_batch_size, num_filters, activation_func, filter_factor, is_data_augment, learning_rate, epochs, is_wandb_log, dropout_factor)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:17:36.225520Z","iopub.execute_input":"2023-04-11T21:17:36.225880Z","iopub.status.idle":"2023-04-11T21:17:49.683944Z","shell.execute_reply.started":"2023-04-11T21:17:36.225851Z","shell.execute_reply":"2023-04-11T21:17:49.682705Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"train_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/train/\ntest_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/val/\nTraining dataset count :  9999\nValidation dataset count 2000\nTraining the Model...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2293577457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdropout_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_data_augment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_wandb_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/2852962611.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_classes, kernel_size, train_path, test_path, train_batch_size, val_batch_size, test_batch_size, num_filters, activation_func, filter_factor, is_data_augment, learning_rate, epochs, is_wandb_log, dropout_factor)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training the Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_wandb_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Finished !!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# train_Loader, val_Loader, test_Loader = load_dataset(is_Data_Augmentation, train_path, test_path, train_batch_size, val_batch_size, test_batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/2582857016.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cnn, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Backward Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Testing on the best hyperparameters","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/inaturalist12k/Data/inaturalist_12K/val/'\ntest_batch_size = 16\ntrain_batch_size = 64\nval_batch_size = 16\nnum_classes = 10\nis_data_augment=True\nis_wandb_log = False\nkernel_size = 3\nnum_filters = 32\nactivation_func = 'Mish'\nfilter_factor = 1\nlearning_rate = 0.001\nepochs = 5\ndropout_factor = 0.3\n\n# test(num_classes, test_path, test_batch_size , num_filters, kernel_size,  activation_func, filter_factor, learning_rate, epochs, dropout_factor)\ncnn = main(num_classes, kernel_size,  train_path, test_path, train_batch_size, val_batch_size, test_batch_size, num_filters, activation_func, filter_factor, is_data_augment, learning_rate, epochs, is_wandb_log, dropout_factor)\ntest(cnn, num_classes, test_path, test_batch_size , num_filters, kernel_size, activation_func, filter_factor, learning_rate, epochs, dropout_factor)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:40:17.265445Z","iopub.execute_input":"2023-04-12T02:40:17.266287Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"train_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/train/\ntest_path /kaggle/input/inaturalist12k/Data/inaturalist_12K/val/\nTraining dataset count :  9999\nValidation dataset count 2000\nTraining the Model...\nEpochs : 1 Train Accuracy : 0.19581958195819582 Train Loss 0.03445962920571842\nEpochs : 1 Validation Accuracy : 0.2825 Validation Loss 0.1275843517780304\nEpochs : 2 Train Accuracy : 0.24592459245924592 Train Loss 0.033002789228698565\nEpochs : 2 Validation Accuracy : 0.2975 Validation Loss 0.1245545185804367\n","output_type":"stream"}]},{"cell_type":"code","source":"test(cnn, num_classes, test_path, test_batch_size , num_filters, kernel_size, activation_func, filter_factor, learning_rate, epochs, dropout_factor)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T02:17:10.929001Z","iopub.execute_input":"2023-04-12T02:17:10.929475Z","iopub.status.idle":"2023-04-12T02:17:11.000698Z","shell.execute_reply.started":"2023-04-12T02:17:10.929432Z","shell.execute_reply":"2023-04-12T02:17:10.998893Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3992703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'cnn' is not defined"],"ename":"NameError","evalue":"name 'cnn' is not defined","output_type":"error"}]}]}